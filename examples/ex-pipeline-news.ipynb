{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOAVBCKK/HPnCbpIGlAzRAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosesyhc/de300-wn2024-notes/blob/main/examples/ex-pipeline-news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google drive for a permanent venv"
      ],
      "metadata": {
        "id": "jWYtZF8BTHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vh-hzOukTYdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving Java, Spark, and `findspark` in Python"
      ],
      "metadata": {
        "id": "Js-a9SWefjV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_3J9C-geuh7"
      },
      "outputs": [],
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install -q seaborn"
      ],
      "metadata": {
        "id": "LKm1xlC-XVDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark setup\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "wMoMrQ-je41K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark helps locate the environment variables\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "DC97uPhMe5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `agnews` Dataset"
      ],
      "metadata": {
        "id": "S7yr9_JzZwQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-wn2024-notes/main/lab/dataset/agnews.csv -O"
      ],
      "metadata": {
        "id": "S51T-4qYZ0LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelining with PySpark MLlib"
      ],
      "metadata": {
        "id": "9WOEde4yY8_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline # pipeline to transform data\n"
      ],
      "metadata": {
        "id": "rWgYKCtGY8vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "PC9bF4RLQvsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "xhcOjvnyqOUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(20)"
      ],
      "metadata": {
        "id": "g5WQzZTcqtSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arrange columns"
      ],
      "metadata": {
        "id": "RGqkLxIDt8L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
        "\n",
        "# Renaming 'Class Index' col to 'label'\n",
        "df = df.withColumnRenamed('Class Index', 'label')\n",
        "\n",
        "# Add a new column 'text' by concatinating 'Title' and 'Description'\n",
        "df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
        "\n",
        "# Remove old text columns\n",
        "df = df.select('label', 'text')\n",
        "\n",
        "# Shows top 10 rows\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "id": "8jgV-jK9snHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "F4s8B9lDuSZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
        "\n",
        "# convert sentences to list of words\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# applies tokenizer to df\n",
        "df = tokenizer.transform(df)\n",
        "\n",
        "df.select(['label','text', 'words']).show(5)\n"
      ],
      "metadata": {
        "id": "iluZMjpnuNnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "_zujVzF9vGQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# remove stopwords\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "\n",
        "# adds a column 'filtered' to df without stopwords\n",
        "df = stopwords_remover.transform(df)\n",
        "\n",
        "df.select(['label','text', 'words', 'filtered']).show(5)"
      ],
      "metadata": {
        "id": "jwqRSHqtu12B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term frequency, Inverse document frequency"
      ],
      "metadata": {
        "id": "7lB7I_H3vSKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "\n",
        "# Calculate term frequency in each article\n",
        "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
        "                       outputCol=\"raw_features\",\n",
        "                       numFeatures=16384)\n",
        "\n",
        "# adds raw tf features to df\n",
        "featurized_data = hashing_tf.transform(df)\n",
        "\n",
        "featurized_data.show(5)"
      ],
      "metadata": {
        "id": "3vfAqoeZvRwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "# inverse document frequency\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "idf_vectorizer = idf.fit(featurized_data)\n",
        "\n",
        "# converting text to vectors\n",
        "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
        "\n",
        "rescaled_data.show(5)"
      ],
      "metadata": {
        "id": "C0bYYaoZvihR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rescaled_data.select('raw_features').show(1, truncate=False)\n",
        "rescaled_data.select('features').show(1, truncate=False)"
      ],
      "metadata": {
        "id": "AwXEZpVswcO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a multinomial logistic regression"
      ],
      "metadata": {
        "id": "macNK1smxX5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train, test) = rescaled_data.randomSplit([0.75, 0.25])\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "metadata": {
        "id": "OeLPS8HwxXXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol='features',\n",
        "                        labelCol='label',\n",
        "                        family=\"multinomial\",\n",
        "                        regParam=0.3,\n",
        "                        elasticNetParam=0,\n",
        "                        maxIter=20)\n",
        "\n",
        "lrModel = lr.fit(train)"
      ],
      "metadata": {
        "id": "01Ml9cUlww1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and evaluation"
      ],
      "metadata": {
        "id": "wpkPVcdnz0Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on test data\n",
        "predictions = lrModel.transform(test)"
      ],
      "metadata": {
        "id": "t880vBmqzswD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.show(3)"
      ],
      "metadata": {
        "id": "JsqzIgId1vf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select(\"text\", 'probability').show()"
      ],
      "metadata": {
        "id": "oVNCsmpw0Onm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select(\"text\", 'probability', 'prediction', 'label').show()"
      ],
      "metadata": {
        "id": "j_0GCx5B1oxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy flag\n",
        "predictions = predictions.withColumn('correctFlag', (col('label') == col('prediction')))"
      ],
      "metadata": {
        "id": "9sCKrPef3MwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.types import FloatType\n",
        "# accuracy\n",
        "predictions.select(avg(col('correctFlag').cast(FloatType())).alias('accuracy')).show()"
      ],
      "metadata": {
        "id": "c7PbHj9D0HUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "# labels = [\"World\", \"Sports\", \"Business\",\"Science\"]\n",
        "\n",
        "# take only the predictions\n",
        "preds_and_labels = predictions.select(['prediction','label']) \\\n",
        "                              .withColumn('label', col('label') \\\n",
        "                              .cast(FloatType())) \\\n",
        "                              .orderBy('prediction')\n",
        "\n",
        "# generate confusion matrix counts\n",
        "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
      ],
      "metadata": {
        "id": "oaiuTIx53cyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "metrics.confusionMatrix().toArray()"
      ],
      "metadata": {
        "id": "xNtcwxpY4REC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelining, from start to finish"
      ],
      "metadata": {
        "id": "UjK_PlwD4fqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
        "\n",
        "def arrangeColumns(df):\n",
        "  # Renaming 'Class Index' col to 'label'\n",
        "  df = df.withColumnRenamed('Class Index', 'label')\n",
        "\n",
        "  # Add a new column 'text' by joining 'Title' and 'Description'\n",
        "  df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
        "\n",
        "  # Select new text feature and labels\n",
        "  df = df.select('label', 'text')\n",
        "  return df\n",
        "\n",
        "df = arrangeColumns(df)\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# stopwords\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "\n",
        "# term frequency\n",
        "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
        "                       outputCol=\"raw_features\",\n",
        "                       numFeatures=16384)\n",
        "\n",
        "# Inverse Document Frequency\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# model\n",
        "lr = LogisticRegression(featuresCol='features',\n",
        "                        labelCol='label',\n",
        "                        family=\"multinomial\",\n",
        "                        regParam=0.3,\n",
        "                        elasticNetParam=0,\n",
        "                        maxIter=20)\n"
      ],
      "metadata": {
        "id": "SYlA_fev4iV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put everything in pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer,\n",
        "                            stopwords_remover,\n",
        "                            hashing_tf,\n",
        "                            idf,\n",
        "                            lr])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(df)\n",
        "\n",
        "# transform add train\n",
        "dataset = pipelineFit.transform(df)"
      ],
      "metadata": {
        "id": "B9_LHoAR5Cc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy\n",
        "dataset = dataset.withColumn('correctFlag', (col('label') == col('prediction')))\n",
        "dataset.select(avg(col('correctFlag').cast(FloatType())).alias('accuracy')).show()"
      ],
      "metadata": {
        "id": "0zuhQmk46PjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUsH_9a36bX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}