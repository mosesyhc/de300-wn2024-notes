{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNNHuiZ1oKKLnEYZXoWy4ne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosesyhc/de300-wn2024-notes/blob/main/examples/ex_rdd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google drive for a permanent venv"
      ],
      "metadata": {
        "id": "jWYtZF8BTHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vh-hzOukTYdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving Java, Spark, and `findspark` in Python"
      ],
      "metadata": {
        "id": "Js-a9SWefjV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_3J9C-geuh7"
      },
      "outputs": [],
      "source": [
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "LKm1xlC-XVDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark setup\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wMoMrQ-je41K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark helps locate the environment variables\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "DC97uPhMe5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD example"
      ],
      "metadata": {
        "id": "IM83bHgKzFFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]  # generic list\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "collection_rdd = sc.parallelize(collection)  # list promoted to RDD\n",
        "\n",
        "print(collection_rdd)"
      ],
      "metadata": {
        "id": "qMAucH--jke2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd.collect()"
      ],
      "metadata": {
        "id": "dUg2cDUCy61L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `map` example"
      ],
      "metadata": {
        "id": "8GjUGvwfzBeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from py4j.protocol import Py4JJavaError\n",
        "\n",
        "def add_one(value):\n",
        "    return value + 1\n",
        "\n",
        "collection_rdd_p1 = collection_rdd.map(add_one)"
      ],
      "metadata": {
        "id": "LxlJGt0uy7V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(collection_rdd_p1.collect())\n",
        "except Py4JJavaError as e:\n",
        "    print(e)\n",
        "\n",
        "# You'll get one of the following:\n",
        "# TypeError: can only concatenate str (not \"int\") to str\n",
        "# TypeError: unsupported operand type(s) for +: 'dict' and 'int'\n",
        "# TypeError: can only concatenate tuple (not \"int\") to tuple"
      ],
      "metadata": {
        "id": "8NhjzPLIzK78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A potential fix"
      ],
      "metadata": {
        "id": "jp6COui11Ihl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safer_add_one(value):\n",
        "    try:\n",
        "        return value + 1\n",
        "    except TypeError:\n",
        "        return value\n",
        "\n",
        "collection_rdd_p1_again = collection_rdd.map(safer_add_one)\n",
        ""
      ],
      "metadata": {
        "id": "rD8LhoWezLvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collection_rdd_p1_again.collect())"
      ],
      "metadata": {
        "id": "568s9Lrd0qR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `filter` example"
      ],
      "metadata": {
        "id": "EhP1bauF1LzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd_filter = collection_rdd.filter(\n",
        "    lambda elem: isinstance(elem, (float, int))\n",
        ")"
      ],
      "metadata": {
        "id": "o6MtLNuO0tzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collection_rdd_filter.collect())"
      ],
      "metadata": {
        "id": "BHPHWaaY14v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add, sub, mul\n",
        "\n",
        "collection_rdd2 = sc.parallelize([4, 7, 9.2, 5.6, -20])"
      ],
      "metadata": {
        "id": "1O-zzGTc17TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd2.reduce(add)"
      ],
      "metadata": {
        "id": "I_7ZHofy3Bnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd2.reduce(\n",
        "    lambda a, b: a + b\n",
        ")"
      ],
      "metadata": {
        "id": "6KvSIcVk3Og6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD and dataframe"
      ],
      "metadata": {
        "id": "UnPMQC2I8ECx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([[1], [2], [3]], schema=[\"column\"])\n",
        "\n",
        "print(df.rdd)"
      ],
      "metadata": {
        "id": "xB9bAZFu8DGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.rdd.collect())"
      ],
      "metadata": {
        "id": "JTTHNxNZ8LXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "- `collection_rdd.count()` returns the number of elements in the RDD.\n",
        "- Reproduce `.count()` using `map`, `filter`, and `reduce`."
      ],
      "metadata": {
        "id": "HRxRMuB_55_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8N83eycY6J1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "- Reproduce our word count example through `map`, `filter`, and `reduce`.\n",
        "\n",
        "*As a reminder, this was the word count code using dataframe*:\n",
        "\n",
        "```\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "counts = (\n",
        "    spark.read.text(file_path)\n",
        "     .select(F.split(F.col('value'), ' ').alias('line'))\n",
        "     .select(F.explode(F.col('line')).alias('word'))\n",
        "     .select(F.lower(F.col('word')).alias('word'))\n",
        "     .select(F.regexp_extract(F.col('word'), r\"(\\W+)?([a-z]+)\", 2).alias('word'))\n",
        "     .where(F.col('word') != \"\")\n",
        "     .groupby('word')\n",
        "     .count()\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "0JeSyNIv6Jm3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yIy-_g84Xpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}