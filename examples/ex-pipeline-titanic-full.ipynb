{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOjzzLo/F8Kti9JrFZTMEcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosesyhc/de300-wn2024-notes/blob/main/examples/ex-pipeline-titanic-full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving Java, Spark, and `findspark` in Python"
      ],
      "metadata": {
        "id": "Js-a9SWefjV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install -q seaborn"
      ],
      "metadata": {
        "id": "LKm1xlC-XVDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "IspG00xAaSEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `titanic` Dataset"
      ],
      "metadata": {
        "id": "S7yr9_JzZwQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic', data_home='dataset/', cache=True)"
      ],
      "metadata": {
        "id": "S51T-4qYZ0LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = spark.read.csv(\n",
        "    \"dataset/titanic.csv\", inferSchema=True, header=True\n",
        ")"
      ],
      "metadata": {
        "id": "8XciBG46a3mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark setup"
      ],
      "metadata": {
        "id": "UE2wGYDta8Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# initiate spark session\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"ML Pipeline\")\n",
        "    .getOrCreate()\n",
        ")\n"
      ],
      "metadata": {
        "id": "PPeQCc_-TPhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.printSchema()"
      ],
      "metadata": {
        "id": "JyUAwLmMgz7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.describe().show()"
      ],
      "metadata": {
        "id": "5z-uCAAIbWFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up duplicate columns\n",
        "titanic = titanic.drop('deck', 'embark_town', 'alive', 'class')"
      ],
      "metadata": {
        "id": "M2YuvvDHb1e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.describe().show()"
      ],
      "metadata": {
        "id": "Ay9UP8cif62O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.select('embarked').distinct().show()"
      ],
      "metadata": {
        "id": "iy0WgIldljmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continuous variables\n",
        "CONTINUOUS_COLUMNS = [\n",
        "    'pclass',\n",
        "    'age',\n",
        "    'sibsp',\n",
        "    'parch',\n",
        "    'fare',\n",
        "]\n",
        "\n",
        "# binary text variables\n",
        "BINARY_COLUMNS = [\n",
        "    'sex'\n",
        "]\n",
        "\n",
        "# categorical variables\n",
        "CATEGORICAL_COLUMNS = [\n",
        "    'embarked',\n",
        "    'who'\n",
        "]\n",
        "\n",
        "# boolean variables\n",
        "BOOLEAN_COLUMNS = [\n",
        "    'adult_male',\n",
        "    'alone'\n",
        "]"
      ],
      "metadata": {
        "id": "aOXmAnOlezuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform boolean columns\n",
        "import pyspark.sql.types as T\n",
        "for x in BOOLEAN_COLUMNS:\n",
        "  titanic = titanic.withColumn(x, titanic[x].cast(T.IntegerType()))"
      ],
      "metadata": {
        "id": "_b03mm7fhr41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimator"
      ],
      "metadata": {
        "id": "vevlahZ9ka7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.ml.feature as MF\n",
        "\n",
        "# transform sex column\n",
        "sex_indexer = MF.StringIndexer(inputCol='sex',\n",
        "                               outputCol='sex_index')\n",
        "\n",
        "# transform who column\n",
        "who_indexer = MF.StringIndexer(inputCol='who',\n",
        "                               outputCol='who_index')\n",
        "who_encoder = MF.OneHotEncoder(inputCol='who_index',\n",
        "                               outputCol='who_vec')\n",
        "\n",
        "# transform embarked column\n",
        "embarked_indexer = MF.StringIndexer(inputCol='embarked',\n",
        "                                  outputCol='embarked_index',\n",
        "                                  handleInvalid='skip')\n",
        "embarked_encoder = MF.OneHotEncoder(inputCol='embarked_index_impute',\n",
        "                                  outputCol='embarked_vec')"
      ],
      "metadata": {
        "id": "_U0XydkVjcCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# age imputation\n",
        "age_imputer  = MF.Imputer(strategy='median',\n",
        "                          inputCol='age',\n",
        "                          outputCol='age')\n",
        "\n",
        "# embarked imputation\n",
        "embarked_imputer = MF.Imputer(strategy='mode',\n",
        "                              inputCol='embarked_index',\n",
        "                              outputCol='embarked_index_impute')"
      ],
      "metadata": {
        "id": "n-z9RhupcQC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "cObdc_AzoYId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collecting all predictors / features\n",
        "assembler = MF.VectorAssembler(inputCols=CONTINUOUS_COLUMNS\n",
        "                               + ['sex_index']\n",
        "                               + ['embarked_vec']\n",
        "                               + ['who_vec'],\n",
        "                               outputCol='features')\n"
      ],
      "metadata": {
        "id": "hK5pZMFroXse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-training pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "F3EHsDfdqPOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "titanic_pipeline = Pipeline(\n",
        "    stages=[\n",
        "        age_imputer,\n",
        "        sex_indexer,\n",
        "        who_indexer,\n",
        "        who_encoder,\n",
        "        embarked_indexer,\n",
        "        embarked_imputer,\n",
        "        embarked_encoder,\n",
        "        assembler\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "xp7k2GRPqVJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembled_titanic = titanic_pipeline.fit(titanic).transform(titanic)"
      ],
      "metadata": {
        "id": "JxljbcRLq3kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembled_titanic.show(10)"
      ],
      "metadata": {
        "id": "kLRyijmPrDfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression model"
      ],
      "metadata": {
        "id": "Q8X-urwep-j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features',labelCol='survived')"
      ],
      "metadata": {
        "id": "wVV4D6Vip9wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_titanic = lr.fit(assembled_titanic)"
      ],
      "metadata": {
        "id": "W4tZtPWiqLvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_pred = lr_titanic.transform(assembled_titanic)"
      ],
      "metadata": {
        "id": "K_ZXWvL4sRMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_pred.show(5)"
      ],
      "metadata": {
        "id": "Q-6vSaCQsdIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "oFSAFa2xsmQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator as bcEval\n",
        "\n",
        "binary_eval = bcEval(rawPredictionCol='prediction',\n",
        "                     labelCol='survived')"
      ],
      "metadata": {
        "id": "AytGHW6PsltC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_eval = binary_eval.evaluate(titanic_pred)"
      ],
      "metadata": {
        "id": "UqPbmpnus2Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic_eval)"
      ],
      "metadata": {
        "id": "YPXWi1QDtbYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the model via extraction of coefficients"
      ],
      "metadata": {
        "id": "WVqJVbD-tvvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_titanic.intercept"
      ],
      "metadata": {
        "id": "QwyE5Majt30F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_titanic.coefficients"
      ],
      "metadata": {
        "id": "koxDhyb3wj_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(CONTINUOUS_COLUMNS\n",
        "+ ['sex_index']\n",
        "+ ['embarked_vec']\n",
        "+ ['who_vec']\n",
        ")"
      ],
      "metadata": {
        "id": "TwU-WtyxwRNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}