{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPH6qI8+rvj3I6AnXdbN7rQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosesyhc/de300-wn2024-notes/blob/main/examples/ex-word-count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google drive for a permanent venv"
      ],
      "metadata": {
        "id": "jWYtZF8BTHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vh-hzOukTYdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving Java, Spark, and `findspark` in Python"
      ],
      "metadata": {
        "id": "Js-a9SWefjV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_3J9C-geuh7"
      },
      "outputs": [],
      "source": [
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "LKm1xlC-XVDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark setup\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wMoMrQ-je41K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark helps locate the environment variables\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "DC97uPhMe5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spark modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "                     .appName(\"Analyzing an unknown article.\")\n",
        "                     .getOrCreate())\n"
      ],
      "metadata": {
        "id": "tSOJwDySfB3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "ZYNX_ybdFY8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext"
      ],
      "metadata": {
        "id": "goDRXwrdfsi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "spark.sparkContext.setLogLevel('WARN')"
      ],
      "metadata": {
        "id": "ATdFAE1gb-Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine file path\n",
        "dir_path = r'/content/drive/MyDrive/DATA_ENG300/'\n",
        "file_path = dir_path + \"sur.txt\""
      ],
      "metadata": {
        "id": "xmXtGU0HgEi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read"
      ],
      "metadata": {
        "id": "mz5axENFg_Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documentation of read\n",
        "spark.read??"
      ],
      "metadata": {
        "id": "3EpDgoyqcHrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read file from spark\n",
        "article = spark.read.text(file_path)  #"
      ],
      "metadata": {
        "id": "F7mT-Ql3b0Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article"
      ],
      "metadata": {
        "id": "AcIWG50HcW9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.printSchema()"
      ],
      "metadata": {
        "id": "ZpqqVERncbA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.show()"
      ],
      "metadata": {
        "id": "cCU-dLO0cpDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# default show arguments\n",
        "article.show(20, truncate=False)"
      ],
      "metadata": {
        "id": "oZQv5SYfcgfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article"
      ],
      "metadata": {
        "id": "4cjoo1o4IkPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieving first rows of the article\n",
        "article.printSchema()"
      ],
      "metadata": {
        "id": "jY-d5iIwgOUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can work with a dataframe by selecting the content\n",
        "article.select(article.value)\n",
        "article.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "QMFHgtiUetDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# all of the following returns the same df\n",
        "article.select(article.value)\n",
        "article.select(article['value'])\n",
        "article.select(col('value'))\n",
        "article.select('value')"
      ],
      "metadata": {
        "id": "ABBKMFR_e9n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split"
      ],
      "metadata": {
        "id": "OD5_z-CZg7kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the lines\n",
        "from pyspark.sql.functions import col, split"
      ],
      "metadata": {
        "id": "71unVBSmd7JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = article.select(split(col('value'), \" \").alias('line'))"
      ],
      "metadata": {
        "id": "rTMGIZIUJzmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "R06vJX9TJzjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.select(split(col('value'), \" \")).printSchema()"
      ],
      "metadata": {
        "id": "wJP8cFSRfEmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.select(split(col('value'), \" \").alias(\"line\")).printSchema()"
      ],
      "metadata": {
        "id": "Ju1jYusqf7gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = article.select(split(article.value, \" \").alias(\"line\"))"
      ],
      "metadata": {
        "id": "8Kf9TtDrf_nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines.printSchema()"
      ],
      "metadata": {
        "id": "zSrAUQU0gbMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines.show()"
      ],
      "metadata": {
        "id": "KcNG7Mydglen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explode / Tokenize"
      ],
      "metadata": {
        "id": "yb6xUIhTg0Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode"
      ],
      "metadata": {
        "id": "VOKsjUkhgnNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))"
      ],
      "metadata": {
        "id": "e67mu5kFhE4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words.show()"
      ],
      "metadata": {
        "id": "0-lMdMPXhKXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words.printSchema()"
      ],
      "metadata": {
        "id": "7m1FTVqAhMsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean"
      ],
      "metadata": {
        "id": "sSU6yFRahpAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))"
      ],
      "metadata": {
        "id": "NK3LgRDohRe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_lower.show(3)"
      ],
      "metadata": {
        "id": "axSSt3REh5zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word_lower\"), r\"(\\W+)?([a-z]+)\", 2).alias(\"word\")\n",
        ")"
      ],
      "metadata": {
        "id": "voVyD2nnh7mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_clean.show(30)"
      ],
      "metadata": {
        "id": "7BtXaJ3Ki71F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
        "\n",
        "words_nonull.show(100)"
      ],
      "metadata": {
        "id": "TEbFJ74RjB8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count / Group"
      ],
      "metadata": {
        "id": "cGZNf_qCkEfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groups = words_nonull.groupBy(col(\"word\"))\n",
        "\n",
        "groups"
      ],
      "metadata": {
        "id": "1CXDxQQYj2hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = words_nonull.groupBy(col(\"word\")).count()"
      ],
      "metadata": {
        "id": "DSEsvgt2kIgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts"
      ],
      "metadata": {
        "id": "hS6OB79tkbzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.orderBy('count', ascending=False).show(15)"
      ],
      "metadata": {
        "id": "ysrZNJv7kcq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.orderBy(\"count\", ascending=False).show(10)"
      ],
      "metadata": {
        "id": "2hooEclCkfU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All in one go!"
      ],
      "metadata": {
        "id": "9MPGWg8dlIeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "counts = (\n",
        "    spark.read.text(file_path)\n",
        "     .select(F.split(F.col('value'), ' ').alias('line'))\n",
        "     .select(F.explode(F.col('line')).alias('word'))\n",
        "     .select(F.lower(F.col('word')).alias('word'))\n",
        "     .select(F.regexp_extract(F.col('word'), r\"(\\W+)?([a-z]+)\", 2).alias('word'))\n",
        "     .where(F.col('word') != \"\")\n",
        "     .groupby('word')\n",
        "     .count()\n",
        ")"
      ],
      "metadata": {
        "id": "of0A-r8Rksap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.orderBy('count', ascending=False).show(10)"
      ],
      "metadata": {
        "id": "BAiPy6Lsl3S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "1. Return the number of words by word length.\n",
        "  - `F.length()` returns the length of a word\n",
        "2. Return the number of each vowel used in the article.\n",
        "  - `filter` / `where`\n",
        "  - `where(F.col('column').isin(['x', 'y', 'z']))` filters so that only values equals x, y, or z remain"
      ],
      "metadata": {
        "id": "fz1J5lDgWLc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "counts = (\n",
        "    spark.read.text(file_path)\n",
        "     .select(F.split(F.col('value'), ' ').alias('line'))\n",
        "     .select(F.explode(F.col('line')).alias('word'))\n",
        "     .select(F.lower(F.col('word')).alias('word'))\n",
        "     .select(F.regexp_extract(F.col('word'), r\"(\\W+)?([a-z]+)\", 2).alias('word'))\n",
        "     .where(F.col('word') != \"\")\n",
        "     .select(F.length(F.col('word')).alias('length'))\n",
        "     .groupby('length')\n",
        "     .count()\n",
        ")"
      ],
      "metadata": {
        "id": "cdDlHusFZmD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.orderBy('length', ascending=True).show(20)"
      ],
      "metadata": {
        "id": "lxg0GO3sabnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "counts = (\n",
        "    spark.read.text(file_path)\n",
        "     .select(F.split(F.col('value'), ' ').alias('line'))\n",
        "     .select(F.explode(F.col('line')).alias('word'))\n",
        "     .select(F.lower(F.col('word')).alias('word'))\n",
        "     .select(F.regexp_extract(F.col('word'), r\"(\\W+)?([a-z]+)\", 2).alias('word'))\n",
        "     .select(F.split(F.col('word'), '').alias('letters'))\n",
        "     .select(F.explode(F.col('letters')).alias('letter'))\n",
        "     .where(F.col('letter') != \"\")\n",
        "     .filter(F.col('letter').isin(['a', 'e', 'i', 'o', 'u']))\n",
        "     .groupby('letter')\n",
        "     .count()\n",
        ")"
      ],
      "metadata": {
        "id": "BdMLT4z_YJeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.show()"
      ],
      "metadata": {
        "id": "-4rKJGCKZbxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch processing"
      ],
      "metadata": {
        "id": "CqBXNyoaUawa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## analyze-article.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "dir_path = r'/content/drive/MyDrive/DATA_ENG300/'\n",
        "file_path = dir_path + \"sur.txt\"  ## \"*.txt\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\n",
        "    \"Counting word occurences from a book.\"\n",
        ").getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
        "results = (\n",
        "    spark.read.text(file_path)\n",
        "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
        "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
        "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
        "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
        "    .where(F.col(\"word\") != \"\")\n",
        "    .groupby(F.col(\"word\"))\n",
        "    .count()\n",
        ")\n",
        "\n",
        "results.orderBy(\"count\", ascending=False).show(10)\n",
        "results.coalesce(1).write.csv(\"./results-analyze-article.csv\")"
      ],
      "metadata": {
        "id": "SD1aEySFl4Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the same code in batch:"
      ],
      "metadata": {
        "id": "103a1LEbjcW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r results-analyze-article.csv/"
      ],
      "metadata": {
        "id": "202-rql_lIil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-3.1.1-bin-hadoop3.2/bin/spark-submit analyze-article.py"
      ],
      "metadata": {
        "id": "7p6v3DxWjapO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMAucH--jke2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}