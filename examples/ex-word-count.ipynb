{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mosesyhc/de300-wn2024-notes/blob/main/examples/ex-word-count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWYtZF8BTHWL"
   },
   "source": [
    "Mounting Google drive for a permanent venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh-hzOukTYdR"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Js-a9SWefjV2"
   },
   "source": [
    "Retrieving Java, Spark, and `findspark` in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_3J9C-geuh7"
   },
   "outputs": [],
   "source": [
    "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.1.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKm1xlC-XVDb"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMoMrQ-je41K"
   },
   "outputs": [],
   "source": [
    "# spark setup\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC97uPhMe5cn"
   },
   "outputs": [],
   "source": [
    "# findspark helps locate the environment variables\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSOJwDySfB3b"
   },
   "outputs": [],
   "source": [
    "# import spark modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .appName(\"Analyzing an unknown article.\")\n",
    "                     .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYNX_ybdFY8_"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goDRXwrdfsi3"
   },
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATdFAE1gb-Ep"
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = spark\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmXtGU0HgEi1"
   },
   "outputs": [],
   "source": [
    "# determine file path\n",
    "dir_path = r'/content/drive/MyDrive/DATA_ENG 300/'\n",
    "file_path = dir_path + \"sur.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz5axENFg_Ub"
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EpDgoyqcHrz"
   },
   "outputs": [],
   "source": [
    "# documentation of read\n",
    "spark.read??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7mT-Ql3b0Ic"
   },
   "outputs": [],
   "source": [
    "# read file from spark\n",
    "article = spark.read.text(file_path)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcIWG50HcW9I"
   },
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpqqVERncbA4"
   },
   "outputs": [],
   "source": [
    "article.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCU-dLO0cpDT"
   },
   "outputs": [],
   "source": [
    "article.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZQv5SYfcgfO"
   },
   "outputs": [],
   "source": [
    "# default show arguments\n",
    "article.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cjoo1o4IkPg"
   },
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY-d5iIwgOUJ"
   },
   "outputs": [],
   "source": [
    "# retrieving first rows of the article\n",
    "article.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMFHgtiUetDS"
   },
   "outputs": [],
   "source": [
    "# we can work with a dataframe by selecting the content\n",
    "article.select(article.value)\n",
    "article.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABBKMFR_e9n6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# all of the following returns the same df\n",
    "article.select(article.value)\n",
    "article.select(article['value'])\n",
    "article.select(col('value'))\n",
    "article.select('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD5_z-CZg7kH"
   },
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71unVBSmd7JN"
   },
   "outputs": [],
   "source": [
    "# splitting the lines\n",
    "from pyspark.sql.functions import col, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTMGIZIUJzmV"
   },
   "outputs": [],
   "source": [
    "lines = article.select(split(col('value'), \" \").alias('line'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R06vJX9TJzjz"
   },
   "outputs": [],
   "source": [
    "lines.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt_YzFtOJzg6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqNzW77NJzR0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJP8cFSRfEmI"
   },
   "outputs": [],
   "source": [
    "article.select(split(col('value'), \" \")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju1jYusqf7gG"
   },
   "outputs": [],
   "source": [
    "article.select(split(col('value'), \" \").alias(\"line\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Kf9TtDrf_nz"
   },
   "outputs": [],
   "source": [
    "lines = article.select(split(article.value, \" \").alias(\"line\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSrAUQU0gbMp"
   },
   "outputs": [],
   "source": [
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcNG7Mydglen"
   },
   "outputs": [],
   "source": [
    "lines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb6xUIhTg0Ji"
   },
   "source": [
    "## Explode / Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOKsjUkhgnNq"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e67mu5kFhE4t"
   },
   "outputs": [],
   "source": [
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-lMdMPXhKXd"
   },
   "outputs": [],
   "source": [
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7m1FTVqAhMsM"
   },
   "outputs": [],
   "source": [
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSU6yFRahpAo"
   },
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK3LgRDohRe4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axSSt3REh5zv"
   },
   "outputs": [],
   "source": [
    "words_lower.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voVyD2nnh7mv"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), r\"(\\W+)?([a-z]+)\", 2).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BtXaJ3Ki71F"
   },
   "outputs": [],
   "source": [
    "words_clean.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEbFJ74RjB8U"
   },
   "outputs": [],
   "source": [
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    "\n",
    "words_nonull.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGZNf_qCkEfL"
   },
   "source": [
    "## Count / Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CXDxQQYj2hh"
   },
   "outputs": [],
   "source": [
    "groups = words_nonull.groupBy(col(\"word\"))\n",
    "\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSEsvgt2kIgs"
   },
   "outputs": [],
   "source": [
    "counts = words_nonull.groupBy(col(\"word\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS6OB79tkbzZ"
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysrZNJv7kcq0"
   },
   "outputs": [],
   "source": [
    "counts.orderBy('count', ascending=False).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hooEclCkfU5"
   },
   "outputs": [],
   "source": [
    "counts.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MPGWg8dlIeF"
   },
   "source": [
    "## All in one go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of0A-r8Rksap"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "counts = (\n",
    "    spark.read.text(file_path)\n",
    "     .select(F.split(F.col('value'), ' ').alias('line'))\n",
    "     .select(F.explode(F.col('line')).alias('word'))\n",
    "     .select(F.lower(F.col('word')).alias('word'))\n",
    "     .select(F.regexp_extract(F.col('word'), r\"(\\W+)?([a-z]+)\", 2).alias('word'))\n",
    "     .where(F.col('word') != \"\")\n",
    "     .groupby('word')\n",
    "     .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAiPy6Lsl3S8"
   },
   "outputs": [],
   "source": [
    "counts.orderBy('count', ascending=False).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD1aEySFl4Ju"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3W1ywLrI6sLni3SKoPGh+",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
