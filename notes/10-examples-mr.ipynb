{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b41bf9-87dd-41c7-980f-0e8640912987",
   "metadata": {},
   "source": [
    "# Examples of MapReduce in machine learning algorithms\n",
    "This document covers selected examples of MapReduce usage when applying machine learning algorithms.\n",
    "\n",
    "## Linear regression\n",
    "Fitting a linear regression:\n",
    "\n",
    "$\\mathbf{y}_{n\\times 1} = \\mathbf{X}_{n\\times p}~\\beta_{p\\times 1} + \\boldsymbol{\\varepsilon}_{n\\times 1}$\n",
    "\n",
    "via least-squares involves \n",
    "\n",
    "$\\min_\\beta \\ell(\\beta) = \\|\\mathbf{y} - \\mathbf{X} \\beta \\|^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdee929-a356-48af-a28e-fc486c5763c1",
   "metadata": {},
   "source": [
    "The corresponding solution is \n",
    "\n",
    "$\\widehat{\\beta} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}\\mathbf{y}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9aec4-1bf9-445b-af87-279c152a3dd2",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- What are the components required to compute the estimated coefficients?\n",
    "- Which step is the slowest?\n",
    "\n",
    "Typically, in a case where linear regression makes sense, $n \\gg p$, such that $\\mathbf{X}^\\mathsf{T}\\mathbf{X}$ is easy to invert.\n",
    "\n",
    "*Note:* Cholesky decomposition is used in practice, for computational stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f6ab4-3d49-4fc2-abac-9a03081e8d30",
   "metadata": {},
   "source": [
    "Notice the required computations are the following:\n",
    "\n",
    "$\\mathbf{X}^\\mathsf{T}\\mathbf{X} = \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\mathsf{T}, \\quad \\mathbf{X}^\\mathsf{T}\\mathbf{y} = \\sum_{i=1}^n \\mathbf{x_i} y_i.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa65f1-ef73-410d-83e5-3703fb522641",
   "metadata": {},
   "source": [
    "`map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975514a-be4c-4289-be14-342256c0e048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f436e06-5659-4519-819a-5c6edc976822",
   "metadata": {},
   "source": [
    "`reduce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7d6bd-98db-4177-934c-7f6db61d72a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a261b08-3c70-48b6-8458-f0c1a0843b37",
   "metadata": {},
   "source": [
    "### [Colab in-class example](https://github.com/mosesyhc/de300-wn2024-notes/blob/main/examples/ex-linear-mr-class.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb81f21-da51-44d9-b441-c3d506266c45",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Fitting a logistic regression typically maximizes the log-likelihood function:\n",
    "\n",
    "$\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log(p(\\mathbf{x}_i)) + (1 - y_i) \\log(1 - p(\\mathbf{x}_i)) \\right], \\quad \\log \\frac{p(\\mathbf{x}_i)}{1 - p(\\mathbf{x}_i)} = \\beta^\\mathsf{T}\\mathbf{x}_i.$\n",
    "\n",
    "Or equivalently,\n",
    "\n",
    "$\\ell(\\beta) = \\sum_{i=1}^n y_i (\\beta^\\mathsf{T}\\mathbf{x}_i) - \\log (1 + \\exp\\{\\beta^\\mathsf{T}\\mathbf{x}_i\\}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d640d6-affd-45c1-b535-5390b7a6207e",
   "metadata": {},
   "source": [
    "How do we design the MapReduce parts for logistic regression?\n",
    "\n",
    "`map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1a2b4-7dd1-464a-91ba-5fc2ba8d6a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a52311e-15eb-4d7f-8948-9521c6658874",
   "metadata": {},
   "source": [
    "`reduce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff3249-7192-4c6a-b13e-caeb5551c4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d88f08d-8cd5-48f5-97e9-44acb5bfb118",
   "metadata": {},
   "source": [
    "### Difference between logistic and linear regression\n",
    "The maximum likelihood estimator $\\widehat{\\beta}$ does not admit a close-formed solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493781c5-a74c-42e1-b448-5637f49d9111",
   "metadata": {},
   "source": [
    "```{figure} ../img/logreg-withSGD.png\n",
    "---\n",
    "width: 80%\n",
    "name: logreg-SGD\n",
    "---\n",
    "MapReduce implementation snippet of logistic regression with SGD.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f88ee9-bb16-4e6f-b880-460cccd230ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
