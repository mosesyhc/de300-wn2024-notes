{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6674b1f0-c64b-4e59-9ae9-54c6c3596868",
   "metadata": {},
   "source": [
    "# Resilient distributed datasets\n",
    "Resilient distributed datasets, or *RDD*s, are the fundamental blocks in PySpark. \n",
    "The associated Colab notebook can be found [here](https://github.com/mosesyhc/de300-wn2024-notes/blob/main/examples/ex-rdd.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6e8da-925a-4410-a2d1-912950d83db8",
   "metadata": {},
   "source": [
    "## What is RDD?  \n",
    "RDD is essentially a collection of unordered objects,\n",
    "- or a mathematical *set*,\n",
    "- or a Python list of objects,\n",
    "- or similar to a JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e33fea-ea42-4eb5-afa1-183dfa9afec3",
   "metadata": {},
   "source": [
    "|<img src=\"../img/rdd-idea.png\" width=\"100%\"/>|<img src=\"../img/math-set.png\" width=\"60%\"/>|<img src=\"../img/json-ex.png\"/>|\n",
    "|-:|:-:|:-|\n",
    "| |Fig. Collection of objects| |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d80b60d-935c-41a1-b28a-03ac73accf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    " \n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]  # generic list\n",
    " \n",
    "sc = spark.sparkContext\n",
    " \n",
    "collection_rdd = sc.parallelize(collection)  # list promoted to RDD\n",
    "\n",
    "print(collection_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c2ade2-a513-4c43-9d22-c509b2be2093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'two', 3.0, ('four', 4), {'five': 5}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collection_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5292397-111c-41b0-92b6-afaacb3108de",
   "metadata": {},
   "source": [
    "## Why RDD if we have dataframes?\n",
    "- If the data at hand are more *freeformed*, using an RDD allows for storage of various types of objects.\n",
    "- Compared to dataframe, which will *attempt* (and fail) to find a common denominator to fit the data above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8928d-738e-4640-a232-0cbd019ac682",
   "metadata": {},
   "source": [
    "## Main ingredients of RDD manipulation\n",
    "We cover three main building blocks for using RDDs, inheriting the concept of a MapReduce scheme.\n",
    "\n",
    "Each of the following building blocks (*functions*) takes a functional input:\n",
    "- `map()`\n",
    "- `filter()`\n",
    "- `reduce()`\n",
    "\n",
    "### `map` through an example\n",
    "`map()` applies the given function to each element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312efecc-4e1a-4291-8eba-003b9ac9062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "def add_one(value):\n",
    "    return value + 1\n",
    "\n",
    "collection_rdd_p1 = collection_rdd.map(add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b073c6-1b0d-4ee2-a0b2-2e27ee90dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    print(collection_rdd_p1.collect())\n",
    "except Py4JJavaError as e:\n",
    "    pass # print(e)\n",
    "\n",
    "# You'll get one of the following:\n",
    "# TypeError: can only concatenate str (not \"int\") to str\n",
    "# TypeError: unsupported operand type(s) for +: 'dict' and 'int'\n",
    "# TypeError: can only concatenate tuple (not \"int\") to tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed71e1-744d-4b95-ae35-afec47d9c081",
   "metadata": {},
   "source": [
    "```{figure} ../img/rdd-map.png\n",
    "---\n",
    "width: 80%\n",
    "name: rdd-map\n",
    "---\n",
    "Applying `add_one()` to each element of RDD through `map()` (Fig 8.2, {cite:p}`rioux2022data`).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3968b1-b0b9-4353-92aa-15a950b11d04",
   "metadata": {},
   "source": [
    "**Quick note:** \n",
    "- Why did the line throw an error?\n",
    "- When was the error thrown?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a97d4-1c2c-4b6b-99ee-0ee5ea465c41",
   "metadata": {},
   "source": [
    "**A potential fix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a233c8-6d1c-44d2-9412-e47fa06cfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safer_add_one(value):\n",
    "    try:\n",
    "        return value + 1\n",
    "    except TypeError:\n",
    "        return value\n",
    "\n",
    "# collection_rdd_p1_again = collection_rdd.map(safer_add_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8eb5c-f035-4a8e-9e07-48374357ea91",
   "metadata": {},
   "source": [
    "**Lesson here**:\n",
    "- PySpark does not warn you about the content of the RDD.\n",
    "- As the developer, we are responsible for how to deal with the data given to an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c940566-96c8-4d69-bc5d-0eb606e04353",
   "metadata": {},
   "source": [
    "### `filter` through an example\n",
    "`filter()` takes a function that returns `True`/`False` based on any conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfff441-752b-49a4-8654-e6e073be1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_rdd_filter = collection_rdd.filter(\n",
    "    lambda elem: isinstance(elem, (float, int))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e7767-2713-40ec-9618-7212ca4496de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(collection_rdd_filter.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0e174-e221-4742-a52b-a135ef0bd7aa",
   "metadata": {},
   "source": [
    "```{figure} ../img/rdd-filter.png\n",
    "---\n",
    "width: 80%\n",
    "name: rdd-filter\n",
    "---\n",
    "Applying `filter()` to the RDD (Fig 8.3, {cite:p}`rioux2022data`).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebe5b4-27f2-4920-885c-c7ede451be48",
   "metadata": {},
   "source": [
    "**A word about `lambda` function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d864eef-7d7f-4b00-8c75-a32c5cee551b",
   "metadata": {},
   "source": [
    "```{figure} ../img/rdd-lambda.png\n",
    "---\n",
    "width: 80%\n",
    "name: rdd-lambda\n",
    "---\n",
    "The use of `lambda` function {cite:p}`rioux2022data`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056d1f7-f27a-4cf6-ba9b-c56163658f2a",
   "metadata": {},
   "source": [
    "### `reduce` through an example\n",
    "`reduce()` summarizes the RDD by sequentially applying the given function.\n",
    "- similar to `groupby()` in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b2328-9c01-4cb3-83a9-e5f076098071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "collection_rdd2 = sc.parallelize([4, 7, 9.2, 5.6, -20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec95197-9069-4685-ad01-1320955448f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection_rdd2.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c173fb-a37c-4eea-b451-67cdc081adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection_rdd2.reduce(\n",
    "#     lambda a, b: a + b\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5db1cb-553e-433c-872f-0454445c9525",
   "metadata": {},
   "source": [
    "```{figure} ../img/rdd-reduce.png\n",
    "---\n",
    "width: 80%\n",
    "name: rdd-reduce\n",
    "---\n",
    "Applying `add` through `reduce()` to the RDD (Fig 8.4, {cite:p}`rioux2022data`).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c305e-3c86-498b-b059-356106643991",
   "metadata": {},
   "source": [
    "**Warnings about `reduce()`**\n",
    "- What functions are reasonable for `reduce()`?\n",
    "- *commutative operation*\n",
    "- *associative operation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782ffc6-3584-4eb6-98cc-45e06433d46a",
   "metadata": {},
   "source": [
    "**Additional Note:**  \n",
    "\n",
    "A dataframe is actually an RDD, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801bee99-595f-4b1a-b6fa-2a8a89d10b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame([[1], [2], [3]], schema=[\"column\"])\n",
    " \n",
    "# print(df.rdd)\n",
    " \n",
    "# print(df.rdd.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
